{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,  # for evaluating the model of a single pixel basis\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.max import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "assert sys.version_info >= (\n",
    "    3,\n",
    "    10,\n",
    "), \"This notebook requires at least Python 3.10\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/CamVid/\")\n",
    "\n",
    "class_labels, X_train_paths, y_train_paths, X_test_paths, y_test_paths, X_val_paths, y_val_paths = locate_data(DATA_DIR, show=True)\n",
    "\n",
    "class_labels['RGB'] = class_labels.apply(lambda x: (x['r'], x['g'], x['b']), axis=1)\n",
    "class_labels = class_labels.drop(['r', 'g', 'b'], axis=1)\n",
    "\n",
    "class_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "X_train = load_data(X_train_paths)\n",
    "y_train = load_data(y_train_paths)\n",
    "\n",
    "assert (\n",
    "    type(X_train[0][0][0][0]) == np.uint8\n",
    "), \"image is not stored as unit8 dtype\"  # PyTorch likes uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "img_idx = 0\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax1.imshow(X_train[img_idx])\n",
    "\n",
    "ax2.set_title(\"Labeled Image\")\n",
    "ax2.imshow(y_train[img_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for pytorch\n",
    "training_batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(dataset=X_train, batch_size=training_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, color_channels = y_train[0].shape\n",
    "print(f\"there are {height * width} pixels in current image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes in current image\n",
    "img_classes = np.unique(y_train[0].reshape(-1, y_train[0].shape[2]), axis=0)\n",
    "img_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO iterate over all images\n",
    "\n",
    "counts = {\n",
    "    \"Animal\": 0,\n",
    "    \"Archway\": 0,\n",
    "    \"Bicyclist\": 0,\n",
    "    \"Bridge\": 0,\n",
    "    \"Building\": 0,\n",
    "    \"Car\": 0,\n",
    "    \"CartLuggagePram\": 0,\n",
    "    \"Child\": 0,\n",
    "    \"Column_Pole\": 0,\n",
    "    \"Fence\": 0,\n",
    "    \"LaneMkgsDriv\": 0,\n",
    "    \"LaneMkgsNonDriv\": 0,\n",
    "    \"Misc_Text\": 0,\n",
    "    \"MotorcycleScooter\": 0,\n",
    "    \"OtherMoving\": 0,\n",
    "    \"ParkingBlock\": 0,\n",
    "    \"Pedestrian\": 0,\n",
    "    \"Road\": 0,\n",
    "    \"RoadShoulder\": 0,\n",
    "    \"Sidewalk\": 0,\n",
    "    \"SignSymbol\": 0,\n",
    "    \"Sky\": 0,\n",
    "    \"SUVPickupTruck\": 0,\n",
    "    \"TrafficCone\": 0,\n",
    "    \"TrafficLight\": 0,\n",
    "    \"Train\": 0,\n",
    "    \"Tree\": 0,\n",
    "    \"Truck_Bus\": 0,\n",
    "    \"Tunnel\": 0,\n",
    "    \"VegetationMisc\": 0,\n",
    "    \"Void\": 0,\n",
    "    \"Wall\": 0,\n",
    "}\n",
    "\n",
    "# count pixels for each class\n",
    "for image in tqdm(y_train):\n",
    "    for c, v in class_labels.to_dict()['RGB'].items():\n",
    "        mask = np.all(image == v, axis=2)  # creates a boolean mask of current class\n",
    "        class_pixel_count = np.sum(mask)\n",
    "        counts.update({c: counts[c] + class_pixel_count})\n",
    "\n",
    "# save counts\n",
    "with open('dev_output/y_train_class_count.pkl', 'wb') as f:\n",
    "    pickle.dump(counts, f)\n",
    "    \n",
    "counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection over Union (IoU) -> commonly used evaluation metric for supervised semantic image segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTH = 1e-6 # To avoid division by zero\n",
    "\n",
    "\n",
    "def iou(model_output: torch.Tensor, label: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate the IoU for a single model output and label pair.\"\"\"\n",
    "    intersection = (\n",
    "        (model_output & label).float().sum((1, 2))\n",
    "    )  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (model_output | label).float().sum((1, 2))  # Will be zero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # smooth division to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.7), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n",
    "\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "\n",
    "    thresholded = torch.clamp(20 * (iou - 0.7), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "\n",
    "    return thresholded  # Or thresholded.mean() if you are interested in average across the batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process:\n",
    "1. calc loss\n",
    "2. calc gradient\n",
    "3. update weights\n",
    "4. repeat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-segmentation-idvBHuu0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
