{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from max import locate_data, load_data\n",
    "from bence import compare_images, augment_images_plain,augment_five_crop\n",
    "from bence import augment_perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SemanticSegmentationDataset(data.Dataset):\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        (\n",
    "            self.class_labels,\n",
    "            self.X_train_paths,\n",
    "            self.y_train_paths,\n",
    "            self.X_test_paths,\n",
    "            self.y_test_paths,\n",
    "            self.X_val_paths,\n",
    "            self.y_val_paths,\n",
    "        ) = locate_data(self.data_path)\n",
    "\n",
    "\n",
    "        # load original training set\n",
    "        self.X_train_orig  = self.load_data(self.X_train_paths)\n",
    "        self.y_train_orig = self.load_data(self.X_train_paths)\n",
    "\n",
    "        print(type(self.X_train_orig))\n",
    "        print(self.X_train_orig.shape)\n",
    "        # flip all images and add to training set\n",
    "        X_train_flipped, y_train_flipped = self.flip_images(self.X_train_orig, self.y_train_orig)\n",
    "        self.X_train = np.concatenate((self.X_train_orig, X_train_flipped))\n",
    "        self.y_train = np.concatenate((self.y_train_orig, y_train_flipped))\n",
    "\n",
    "        # five crop images with no_augmentations = 10 and add to training set\n",
    "        X_train_crop, y_train_crop = self.five_crop_images(self.X_train_orig, self.y_train_orig, no_augmentations=10)\n",
    "        self.X_train = np.concatenate((self.X_train, X_train_crop))\n",
    "        self.y_train = np.concatenate((self.y_train, y_train_crop))\n",
    "\n",
    "        # augment_perspective no_aumentaions = 20 and add to training set\n",
    "        X_train_persp, y_train_persp = self.augment_perspective(self.X_train_orig, self.y_train_orig, no_augmentations=20)\n",
    "        self.X_train = np.concatenate((self.X_train, X_train_persp))\n",
    "        self.y_train = np.concatenate((self.y_train, y_train_persp))\n",
    "\n",
    "        # T.ColorJitter(brightness=(1,2),hue=(-0.2,0.5)) with augment_images_plain 5 times on 10 images and add to training set\n",
    "        X_train_jittered, y_train_jittered = self.augment_images_plain(self.X_train_orig, self.y_train_orig, \n",
    "                                                                       T.ColorJitter(brightness=(1,2),hue=(-0.2,0.5)),\n",
    "                                                                       no_augmentations=5)\n",
    "        self.X_train = np.concatenate((self.X_train, X_train_jittered))\n",
    "        self.y_train = np.concatenate((self.y_train, y_train_jittered))\n",
    "\n",
    "        X_train_jittered, y_train_jittered = self.augment_images_plain(self.X_train_orig, self.y_train_orig, \n",
    "                                                                       T.ColorJitter(brightness=(1,2),hue=(-0.2,0.5)),\n",
    "                                                                       no_augmentations=5)\n",
    "        self.X_train = np.concatenate((self.X_train, X_train_jittered))\n",
    "        self.y_train = np.concatenate((self.y_train, y_train_jittered))\n",
    "        X_train_jittered, y_train_jittered = self.augment_images_plain(self.X_train_orig, self.y_train_orig, \n",
    "                                                                       T.ColorJitter(brightness=(1,2),hue=(-0.2,0.5)),\n",
    "                                                                       no_augmentations=5)\n",
    "        self.X_train = np.concatenate((self.X_train, X_train_jittered))\n",
    "        self.y_train = np.concatenate((self.y_train, y_train_jittered))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X = Image.open(self.X_train[index])\n",
    "        y = Image.open(self.y_train[index])\n",
    "        X = self.transforms(X)\n",
    "        y = np.array(y, dtype=np.int64)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def load_data(self,file_paths: list[Path]) -> np.ndarray:\n",
    "        \"\"\"Load images into memory with uint8 dtype (format PyTorch likes\"\"\"\n",
    "        array= np.array([np.array(Image.open(x), dtype=np.uint8) for x in file_paths])\n",
    "        tensor = torch.from_numpy(array).permute(0,3,1,2)\n",
    "        return tensor\n",
    "    \n",
    "    def flip_images(self,X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Flip images horizontally\"\"\"\n",
    "        X_flipped = torch.flip(X, dims=[3])\n",
    "        y_flipped = torch.flip(y, dims=[3])\n",
    "        return X_flipped, y_flipped\n",
    "    \n",
    "    def five_crop_images(self,X: torch.Tensor, y: torch.Tensor, no_augmentations = 10,crop_size = (360, 480)) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Five crop images\"\"\"\n",
    "        five_cropper = T.FiveCrop(size=crop_size)\n",
    "        resizer = T.Resize((720, 960))\n",
    "        samples = np.random.randint(X.shape[0], size = no_augmentations)\n",
    "        X_cropped = five_cropper(X[samples])\n",
    "        y_cropped = five_cropper(y[samples])\n",
    "        X_cropped = torch.cat([resizer(x) for x in X_cropped])\n",
    "        y_cropped = torch.cat([resizer(x) for x in y_cropped])\n",
    "        return X_cropped, y_cropped\n",
    "    \n",
    "    def augment_perspective(self,X: torch.Tensor, y: torch.Tensor, distortions_scale= 0.6,p = 1.0, no_augmentations = 20) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Augment perspective\"\"\"\n",
    "        samples = np.random.randint(X.shape[0], size = no_augmentations)\n",
    "        perspective = T.RandomPerspective(distortion_scale=distortions_scale, p=1.0)\n",
    "        joint_tensor = torch.cat([X[samples], y[samples]])\n",
    "        joint_tensor = perspective(joint_tensor)\n",
    "        X_persp = joint_tensor[:no_augmentations]\n",
    "        y_persp = joint_tensor[no_augmentations:]\n",
    "        return X_persp, y_persp\n",
    "    \n",
    "    def augment_images_plain(self,X: torch.Tensor, y: torch.Tensor, transform: T, no_augmentations = 5) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Augment images with plain augmentation\"\"\"\n",
    "        samples = np.random.randint(X.shape[0], size = no_augmentations)\n",
    "        X_augmented = transform(X[samples])\n",
    "        y_augmented = y[samples]\n",
    "\n",
    "        return X_augmented, y_augmented\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([369, 3, 720, 960])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DATA_DIR \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39m../data/CamVid/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m SemanticSegmentationDataset(DATA_DIR)\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mSemanticSegmentationDataset.__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train_orig, y_train_flipped))\n\u001b[1;32m     30\u001b[0m \u001b[39m# five crop images with no_augmentations = 10 and add to training set\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m X_train_crop, y_train_crop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfive_crop_images(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train_orig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train_orig, no_augmentations\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train, X_train_crop))\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train, y_train_crop))\n",
      "Cell \u001b[0;32mIn[13], line 74\u001b[0m, in \u001b[0;36mSemanticSegmentationDataset.five_crop_images\u001b[0;34m(self, X, y, no_augmentations, crop_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m five_cropper \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mFiveCrop(size\u001b[39m=\u001b[39mcrop_size)\n\u001b[1;32m     73\u001b[0m resizer \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mResize((\u001b[39m720\u001b[39m, \u001b[39m960\u001b[39m))\n\u001b[0;32m---> 74\u001b[0m samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(X_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], size \u001b[39m=\u001b[39m no_augmentations)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../data/CamVid/\")\n",
    "dataset = SemanticSegmentationDataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-segmentation-QN7uw8Fi-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
